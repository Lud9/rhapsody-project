{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Original code: [GITHUB](https://github.com/pyduan/amazonaccess/blob/master/BSMan/ensemble.py)\n",
        "\n",
        "    Code to label test data"
      ],
      "metadata": {
        "id": "6YIhnkfWdD-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array, hstack\n",
        "from sklearn import metrics, linear_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import naive_bayes\n",
        "from sklearn import preprocessing\n",
        "from scipy import sparse\n",
        "from itertools import combinations\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "\n",
        "SEED = 55"
      ],
      "metadata": {
        "id": "Lp7P0Jt4m0Zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "traindata = pd.read_csv('/data.csv')\n",
        "testdata = pd.read_csv('/test.csv')"
      ],
      "metadata": {
        "id": "vke5X2T2muCq",
        "outputId": "9015215e-a6b0-437e-a98b-7dc4678e32ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1355276234.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraindata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtestdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This program is based on code submitted by Miroslaw Horbal to the Kaggle\n",
        "forums, which was itself based on an earlier submission from Paul Doan.\n",
        "My thanks to both.\n",
        "\n",
        "Author: Benjamin Solecki <bensolucky@gmail.com>\n",
        "\"\"\"\n",
        "\n",
        "def group_data(data, degree=3, hash=hash):\n",
        "    new_data = []\n",
        "    m, n = data.shape\n",
        "    for indicies in combinations(range(n), degree):\n",
        "        if 5 in indicies and 7 in indicies:\n",
        "            print(\"feature Xd\")\n",
        "        elif 2 in indicies and 3 in indicies:\n",
        "            print(\"feature Xd\")\n",
        "        else:\n",
        "            new_data.append([hash(tuple(v)) for v in data[:, indicies]])\n",
        "    return array(new_data).T\n",
        "\n",
        "def OneHotEncoder(data, keymap=None):\n",
        "    if keymap is None:\n",
        "        keymap = []\n",
        "        for col in data.T:\n",
        "            uniques = set(list(col))\n",
        "            keymap.append(dict((key, i) for i, key in enumerate(uniques)))\n",
        "    total_pts = data.shape[0]\n",
        "    outdat = []\n",
        "    for i, col in enumerate(data.T):\n",
        "        km = keymap[i]\n",
        "        num_labels = len(km)\n",
        "        spmat = sparse.lil_matrix((total_pts, num_labels))\n",
        "        for j, val in enumerate(col):\n",
        "            if val in km:\n",
        "                spmat[j, km[val]] = 1\n",
        "        outdat.append(spmat)\n",
        "    outdat = sparse.hstack(outdat).tocsr()\n",
        "    return outdat, keymap\n",
        "\n",
        "def create_test_submission(filename, prediction):\n",
        "    content = ['id,ACTION']\n",
        "    for i, p in enumerate(prediction):\n",
        "        content.append('%i,%f' % (i+1, p))\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write('\\n'.join(content))\n",
        "    print('Saved')\n",
        "\n",
        "def cv_loop(X, y, model, N):\n",
        "    mean_auc = 0.\n",
        "    for i in range(N):\n",
        "        X_train, X_cv, y_train, y_cv = train_test_split(\n",
        "            X, y, test_size=1.0 / float(N),\n",
        "            random_state=i * SEED)\n",
        "        model.fit(X_train, y_train)\n",
        "        preds = model.predict_proba(X_cv)[:, 1]\n",
        "        auc = metrics.roc_auc_score(y_cv, preds)\n",
        "        mean_auc += auc\n",
        "    return mean_auc / N\n",
        "\n",
        "# Define learner\n",
        "learner = sys.argv[1]\n",
        "print(\"Reading dataset...\")\n",
        "\n",
        "train_data = traindata\n",
        "test_data = testdata\n",
        "submit = learner + str(SEED) + '.csv'\n",
        "\n",
        "all_data = np.vstack((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:-1]))\n",
        "num_train = train_data.shape[0]\n",
        "\n",
        "print(\"Transforming data...\")\n",
        "relabler = preprocessing.LabelEncoder()\n",
        "for col in range(all_data.shape[1]):\n",
        "    relabler.fit(all_data[:, col])\n",
        "    all_data[:, col] = relabler.transform(all_data[:, col])\n",
        "\n",
        "dp = group_data(all_data, degree=2)\n",
        "for col in range(dp.shape[1]):\n",
        "    relabler.fit(dp[:, col])\n",
        "    dp[:, col] = relabler.transform(dp[:, col])\n",
        "    uniques = len(set(dp[:, col]))\n",
        "    maximum = max(dp[:, col])\n",
        "    if maximum < 65534:\n",
        "        count_map = np.bincount(dp[:, col].astype('uint16'))\n",
        "        for n, i in enumerate(dp[:, col]):\n",
        "            if count_map[i] <= 1:\n",
        "                dp[n, col] = uniques\n",
        "            elif count_map[i] == 2:\n",
        "                dp[n, col] = uniques + 1\n",
        "    else:\n",
        "        for n, i in enumerate(dp[:, col]):\n",
        "            if (dp[:, col] == i).sum() <= 1:\n",
        "                dp[n, col] = uniques\n",
        "            elif (dp[:, col] == i).sum() == 2:\n",
        "                dp[n, col] = uniques + 1\n",
        "    relabler.fit(dp[:, col])\n",
        "    dp[:, col] = relabler.transform(dp[:, col])\n",
        "\n",
        "dt = group_data(all_data, degree=3)\n",
        "for col in range(dt.shape[1]):\n",
        "    relabler.fit(dt[:, col])\n",
        "    dt[:, col] = relabler.transform(dt[:, col])\n",
        "    uniques = len(set(dt[:, col]))\n",
        "    maximum = max(dt[:, col])\n",
        "    if maximum < 65534:\n",
        "        count_map = np.bincount(dt[:, col].astype('uint16'))\n",
        "        for n, i in enumerate(dt[:, col]):\n",
        "            if count_map[i] <= 1:\n",
        "                dt[n, col] = uniques\n",
        "            elif count_map[i] == 2:\n",
        "                dt[n, col] = uniques + 1\n",
        "    else:\n",
        "        for n, i in enumerate(dt[:, col]):\n",
        "            if (dt[:, col] == i).sum() <= 1:\n",
        "                dt[n, col] = uniques\n",
        "            elif (dt[:, col] == i).sum() == 2:\n",
        "                dt[n, col] = uniques + 1\n",
        "    relabler.fit(dt[:, col])\n",
        "    dt[:, col] = relabler.transform(dt[:, col])\n",
        "\n",
        "for col in range(all_data.shape[1]):\n",
        "    relabler.fit(all_data[:, col])\n",
        "    all_data[:, col] = relabler.transform(all_data[:, col])\n",
        "    uniques = len(set(all_data[:, col]))\n",
        "    maximum = max(all_data[:, col])\n",
        "    if maximum < 65534:\n",
        "        count_map = np.bincount(all_data[:, col].astype('uint16'))\n",
        "        for n, i in enumerate(all_data[:, col]):\n",
        "            if count_map[i] <= 1:\n",
        "                all_data[n, col] = uniques\n",
        "            elif count_map[i] == 2:\n",
        "                all_data[n, col] = uniques + 1\n",
        "    else:\n",
        "        for n, i in enumerate(all_data[:, col]):\n",
        "            if (all_data[:, col] == i).sum() <= 1:\n",
        "                all_data[n, col] = uniques\n",
        "            elif (all_data[:, col] == i).sum() == 2:\n",
        "                all_data[n, col] = uniques + 1\n",
        "    relabler.fit(all_data[:, col])\n",
        "    all_data[:, col] = relabler.transform(all_data[:, col])\n",
        "\n",
        "# Prepare features\n",
        "y = array(train_data.ACTION)\n",
        "X = all_data[:num_train]\n",
        "X_2 = dp[:num_train]\n",
        "X_3 = dt[:num_train]\n",
        "X_test = all_data[num_train:]\n",
        "X_test_2 = dp[num_train:]\n",
        "X_test_3 = dt[num_train:]\n",
        "\n",
        "X_train_all = np.hstack((X, X_2, X_3))\n",
        "X_test_all = np.hstack((X_test, X_test_2, X_test_3))\n",
        "num_features = X_train_all.shape[1]\n",
        "\n",
        "# === Use predefined good features ===\n",
        "good_features = [0, 7, 8, 9, 28, 36, 40, 58, 59, 60, 62, 64, 69]\n",
        "print(\"Selected features %s\" % good_features)\n",
        "with open(\"feats\" + submit, 'w') as gf:\n",
        "    print(gf, good_features)\n",
        "print(len(good_features), \" features\")\n",
        "\n",
        "# Choose model\n",
        "if learner == 'NB':\n",
        "    model = naive_bayes.BernoulliNB(alpha=0.03)\n",
        "else:\n",
        "    model = linear_model.LogisticRegression(class_weight='balanced', penalty='l2', solver='lbfgs', max_iter=50000)\n",
        "\n",
        "\n",
        "Xts = [OneHotEncoder(X_train_all[:, [i]])[0] for i in range(num_features)]\n",
        "\n",
        "# === Hyperparameter tuning ===\n",
        "print(\"Performing hyperparameter selection...\")\n",
        "Xt = sparse.hstack([Xts[j] for j in good_features]).tocsr()\n",
        "N = 10\n",
        "score_hist = []\n",
        "\n",
        "if learner == 'NB':\n",
        "    Cvals = [0.001, 0.003, 0.006, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.1]\n",
        "else:\n",
        "    Cvals = np.logspace(-4, 4, 15, base=2)\n",
        "\n",
        "for C in Cvals:\n",
        "    if learner == 'NB':\n",
        "        model.alpha = C\n",
        "    else:\n",
        "        model.C = C\n",
        "    score = cv_loop(Xt, y, model, N)\n",
        "    score_hist.append((score, C))\n",
        "    print(\"C: %f Mean AUC: %f\" % (C, score))\n",
        "\n",
        "bestC = sorted(score_hist)[-1][1]\n",
        "print(\"Best C value: %f\" % bestC)\n",
        "\n",
        "print(\"Performing One Hot Encoding on entire dataset...\")\n",
        "Xt = np.vstack((X_train_all[:, good_features], X_test_all[:, good_features]))\n",
        "Xt, keymap = OneHotEncoder(Xt)\n",
        "X_train = Xt[:num_train]\n",
        "X_test = Xt[num_train:]\n",
        "\n",
        "if learner == 'NB':\n",
        "    model.alpha = bestC\n",
        "else:\n",
        "    model.C = bestC\n",
        "\n",
        "print(\"Training full model...\")\n",
        "model.fit(X_train, y)\n",
        "\n",
        "print(\"Making prediction and saving results...\")\n",
        "preds = model.predict_proba(X_test)[:, 1]\n",
        "create_test_submission(submit, preds)\n",
        "\n",
        "preds = model.predict_proba(X_train)[:, 1]\n",
        "create_test_submission('Train' + submit, preds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "KJcK6aivY6lJ",
        "outputId": "fdb21249-8004-47f4-9ac9-cf35502f2f7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading dataset...\n",
            "Transforming data...\n",
            "feature Xd\n",
            "feature Xd\n",
            "feature Xd\n",
            "feature Xd\n",
            "feature Xd\n",
            "feature Xd\n",
            "feature Xd\n",
            "feature Xd\n",
            "feature Xd\n",
            "feature Xd\n",
            "feature Xd\n",
            "feature Xd\n",
            "feature Xd\n",
            "feature Xd\n",
            "Selected features [0, 7, 8, 9, 28, 36, 40, 46, 58, 59, 60, 62, 64, 69]\n",
            "<_io.TextIOWrapper name='feats-f55.csv' mode='w' encoding='utf-8'> [0, 7, 8, 9, 28, 36, 40, 46, 58, 59, 60, 62, 64, 69]\n",
            "14  features\n",
            "Performing hyperparameter selection...\n",
            "C: 0.062500 Mean AUC: 0.902358\n",
            "C: 0.092875 Mean AUC: 0.905785\n",
            "C: 0.138011 Mean AUC: 0.907839\n",
            "C: 0.205084 Mean AUC: 0.908916\n",
            "C: 0.304753 Mean AUC: 0.909250\n",
            "C: 0.452862 Mean AUC: 0.909185\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-3085549982.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m     \u001b[0mscore_hist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"C: %f Mean AUC: %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3-3085549982.py\u001b[0m in \u001b[0;36mcv_loop\u001b[0;34m(X, y, model, N)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             random_state=i * SEED)\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_cv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1348\u001b[0m             \u001b[0mn_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n\u001b[0m\u001b[1;32m   1351\u001b[0m             path_func(\n\u001b[1;32m   1352\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1984\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1985\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1986\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1988\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1912\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1914\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1915\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[1;32m    449\u001b[0m                 \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             ]\n\u001b[0;32m--> 451\u001b[0;31m             opt_res = optimize.minimize(\n\u001b[0m\u001b[1;32m    452\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0mw0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    736\u001b[0m                                  **options)\n\u001b[1;32m    737\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m         res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0m\u001b[1;32m    739\u001b[0m                                callback=callback, **options)\n\u001b[1;32m    740\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/optimize/_lbfgsb_py.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0;31m# x, f, g, wa, iwa, task, csave, lsave, isave, dsave = \\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         _lbfgsb.setulb(m, x, low_bnd, upper_bnd, nbd, f, g, factr, pgtol, wa,\n\u001b[0m\u001b[1;32m    434\u001b[0m                        iwa, task, lsave, isave, dsave, maxls, ln_task)\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1st Version to mine Selected features\n",
        "    [0, 7, 8, 9, 28, 36, 40, 46, 58, 59, 60, 62, 64, 69]"
      ],
      "metadata": {
        "id": "RMcxHSDlnHxq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "xqOC7VlQlJSc",
        "outputId": "aa827873-144c-4fbc-9c4b-c76b9e3aa3ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading dataset...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2593486580.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0msubmit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearner\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0mnum_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "This program is based on code submitted by Miroslaw Horbal to the Kaggle\n",
        "forums, which was itself based on an earlier submission from Paul Doan.\n",
        "My thanks to both.\n",
        "\n",
        "Author: Benjamin Solecki <bensolucky@gmail.com>\n",
        "\"\"\"\n",
        "\n",
        "from numpy import array, hstack\n",
        "from sklearn import metrics, linear_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import naive_bayes\n",
        "from sklearn import preprocessing\n",
        "from scipy import sparse\n",
        "from itertools import combinations\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "\n",
        "SEED = 55\n",
        "\n",
        "def group_data(data, degree=3, hash=hash):\n",
        "    \"\"\"\n",
        "    numpy.array -> numpy.array\n",
        "\n",
        "    Groups all columns of data into all combinations of triples\n",
        "    \"\"\"\n",
        "    new_data = []\n",
        "    m,n = data.shape\n",
        "    for indicies in combinations(range(n), degree):\n",
        "        if 5 in indicies and 7 in indicies:\n",
        "            print(\"feature Xd\")\n",
        "        elif 2 in indicies and 3 in indicies:\n",
        "            print(\"feature Xd\")\n",
        "        else:\n",
        "            new_data.append([hash(tuple(v)) for v in data[:,indicies]])\n",
        "    return array(new_data).T\n",
        "\n",
        "def OneHotEncoder(data, keymap=None):\n",
        "     \"\"\"\n",
        "     OneHotEncoder takes data matrix with categorical columns and\n",
        "     converts it to a sparse binary matrix.\n",
        "\n",
        "     Returns sparse binary matrix and keymap mapping categories to indicies.\n",
        "     If a keymap is supplied on input it will be used instead of creating one\n",
        "     and any categories appearing in the data that are not in the keymap are\n",
        "     ignored\n",
        "     \"\"\"\n",
        "     if keymap is None:\n",
        "          keymap = []\n",
        "          for col in data.T:\n",
        "               uniques = set(list(col))\n",
        "               keymap.append(dict((key, i) for i, key in enumerate(uniques)))\n",
        "     total_pts = data.shape[0]\n",
        "     outdat = []\n",
        "     for i, col in enumerate(data.T):\n",
        "          km = keymap[i]\n",
        "          num_labels = len(km)\n",
        "          spmat = sparse.lil_matrix((total_pts, num_labels))\n",
        "          for j, val in enumerate(col):\n",
        "               if val in km:\n",
        "                    spmat[j, km[val]] = 1\n",
        "          outdat.append(spmat)\n",
        "     outdat = sparse.hstack(outdat).tocsr()\n",
        "     return outdat, keymap\n",
        "\n",
        "def create_test_submission(filename, prediction):\n",
        "    content = ['id,ACTION']\n",
        "    for i, p in enumerate(prediction):\n",
        "        content.append('%i,%f' %(i+1,p))\n",
        "    f = open(filename, 'w')\n",
        "    f.write('\\n'.join(content))\n",
        "    f.close()\n",
        "    print('Saved')\n",
        "\n",
        "# This loop essentially from Paul's starter code\n",
        "# I (Ben) increased the size of train at the expense of test, because\n",
        "# when train is small many features will not be found in train.\n",
        "def cv_loop(X, y, model, N):\n",
        "    mean_auc = 0.\n",
        "    for i in range(N):\n",
        "        X_train, X_cv, y_train, y_cv = train_test_split(\n",
        "                                       X, y, test_size=1.0/float(N),\n",
        "                                       random_state = i*SEED)\n",
        "        model.fit(X_train, y_train)\n",
        "        preds = model.predict_proba(X_cv)[:,1]\n",
        "        auc = metrics.roc_auc_score(y_cv, preds)\n",
        "        #print \"AUC (fold %d/%d): %f\" % (i + 1, N, auc)\n",
        "        mean_auc += auc\n",
        "    return mean_auc/N\n",
        "\n",
        "learner = sys.argv[1]\n",
        "print(\"Reading dataset...\")\n",
        "\n",
        "submit=learner + str(SEED) + '.csv'\n",
        "all_data = np.vstack((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:-1]))\n",
        "num_train = np.shape(train_data)[0]\n",
        "\n",
        "# Transform data\n",
        "print(\"Transforming data...\")\n",
        "# Relabel the variable values to smallest possible so that I can use bincount\n",
        "# on them later.\n",
        "relabler = preprocessing.LabelEncoder()\n",
        "for col in range(len(all_data[0,:])):\n",
        "    relabler.fit(all_data[:, col])\n",
        "    all_data[:, col] = relabler.transform(all_data[:, col])\n",
        "\n",
        "########################## 2nd order features ################################\n",
        "dp = group_data(all_data, degree=2)\n",
        "for col in range(len(dp[0,:])):\n",
        "    relabler.fit(dp[:, col])\n",
        "    dp[:, col] = relabler.transform(dp[:, col])\n",
        "    uniques = len(set(dp[:,col]))\n",
        "    maximum = max(dp[:,col])\n",
        "    print(col)\n",
        "    if maximum < 65534:\n",
        "        count_map = np.bincount((dp[:, col]).astype('uint16'))\n",
        "        for n,i in enumerate(dp[:, col]):\n",
        "            if count_map[i] <= 1:\n",
        "                dp[n, col] = uniques\n",
        "            elif count_map[i] == 2:\n",
        "                dp[n, col] = uniques+1\n",
        "    else:\n",
        "        for n,i in enumerate(dp[:, col]):\n",
        "            if (dp[:, col] == i).sum() <= 1:\n",
        "                dp[n, col] = uniques\n",
        "            elif (dp[:, col] == i).sum() == 2:\n",
        "                dp[n, col] = uniques+1\n",
        "    print(uniques) # unique values\n",
        "    uniques = len(set(dp[:,col]))\n",
        "    print(uniques)\n",
        "    relabler.fit(dp[:, col])\n",
        "    dp[:, col] = relabler.transform(dp[:, col])\n",
        "\n",
        "########################## 3rd order features ################################\n",
        "dt = group_data(all_data, degree=3)\n",
        "for col in range(len(dt[0,:])):\n",
        "    relabler.fit(dt[:, col])\n",
        "    dt[:, col] = relabler.transform(dt[:, col])\n",
        "    uniques = len(set(dt[:,col]))\n",
        "    maximum = max(dt[:,col])\n",
        "    print(col)\n",
        "    if maximum < 65534:\n",
        "        count_map = np.bincount((dt[:, col]).astype('uint16'))\n",
        "        for n,i in enumerate(dt[:, col]):\n",
        "            if count_map[i] <= 1:\n",
        "                dt[n, col] = uniques\n",
        "            elif count_map[i] == 2:\n",
        "                dt[n, col] = uniques+1\n",
        "    else:\n",
        "        for n,i in enumerate(dt[:, col]):\n",
        "            if (dt[:, col] == i).sum() <= 1:\n",
        "                dt[n, col] = uniques\n",
        "            elif (dt[:, col] == i).sum() == 2:\n",
        "                dt[n, col] = uniques+1\n",
        "    print(uniques)\n",
        "    uniques = len(set(dt[:,col]))\n",
        "    print(uniques)\n",
        "    relabler.fit(dt[:, col])\n",
        "    dt[:, col] = relabler.transform(dt[:, col])\n",
        "\n",
        "########################## 1st order features ################################\n",
        "for col in range(len(all_data[0,:])):\n",
        "    relabler.fit(all_data[:, col])\n",
        "    all_data[:, col] = relabler.transform(all_data[:, col])\n",
        "    uniques = len(set(all_data[:,col]))\n",
        "    maximum = max(all_data[:,col])\n",
        "    print(col)\n",
        "    if maximum < 65534:\n",
        "        count_map = np.bincount((all_data[:, col]).astype('uint16'))\n",
        "        for n,i in enumerate(all_data[:, col]):\n",
        "            if count_map[i] <= 1:\n",
        "                all_data[n, col] = uniques\n",
        "            elif count_map[i] == 2:\n",
        "                all_data[n, col] = uniques+1\n",
        "    else:\n",
        "        for n,i in enumerate(all_data[:, col]):\n",
        "            if (all_data[:, col] == i).sum() <= 1:\n",
        "                all_data[n, col] = uniques\n",
        "            elif (all_data[:, col] == i).sum() == 2:\n",
        "                all_data[n, col] = uniques+1\n",
        "    print(uniques)\n",
        "    uniques = len(set(all_data[:,col]))\n",
        "    print(uniques)\n",
        "    relabler.fit(all_data[:, col])\n",
        "    all_data[:, col] = relabler.transform(all_data[:, col])\n",
        "\n",
        "# Collect the training features together\n",
        "y = array(train_data.ACTION)\n",
        "X = all_data[:num_train]\n",
        "X_2 = dp[:num_train]\n",
        "X_3 = dt[:num_train]\n",
        "\n",
        "# Collect the testing features together\n",
        "X_test = all_data[num_train:]\n",
        "X_test_2 = dp[num_train:]\n",
        "X_test_3 = dt[num_train:]\n",
        "\n",
        "X_train_all = np.hstack((X, X_2, X_3))\n",
        "X_test_all = np.hstack((X_test, X_test_2, X_test_3))\n",
        "num_features = X_train_all.shape[1]\n",
        "\n",
        "if learner == 'NB':\n",
        "    model = naive_bayes.BernoulliNB(alpha=0.03)\n",
        "else:\n",
        "    # model = linear_model.LogisticRegression(class_weight='auto', penalty='l2')\n",
        "    model = linear_model.LogisticRegression(class_weight='balanced', penalty='l2')\n",
        "\n",
        "# Xts holds one hot encodings for each individual feature in memory\n",
        "# speeding up feature selection\n",
        "Xts = [OneHotEncoder(X_train_all[:,[i]])[0] for i in range(num_features)]\n",
        "\n",
        "print(\"Performing greedy feature selection...\")\n",
        "score_hist = []\n",
        "N = 10\n",
        "good_features = set([])\n",
        "# Greedy feature selection loop\n",
        "while len(score_hist) < 2 or score_hist[-1][0] > score_hist[-2][0]:\n",
        "    scores = []\n",
        "    for f in range(len(Xts)):\n",
        "        if f not in good_features:\n",
        "            feats = list(good_features) + [f]\n",
        "            Xt = sparse.hstack([Xts[j] for j in feats]).tocsr()\n",
        "            score = cv_loop(Xt, y, model, N)\n",
        "            scores.append((score, f))\n",
        "            print(\"Feature: %i Mean AUC: %f\" % (f, score))\n",
        "    good_features.add(sorted(scores)[-1][1])\n",
        "    score_hist.append(sorted(scores)[-1])\n",
        "    print(\"Current features: %s\" % sorted(list(good_features)))\n",
        "\n",
        "# Remove last added feature from good_features\n",
        "good_features.remove(score_hist[-1][1])\n",
        "good_features = sorted(list(good_features))\n",
        "\n",
        "print(\"Selected features %s\" % good_features)\n",
        "gf = open(\"feats\" + submit, 'w')\n",
        "print(gf, good_features)\n",
        "gf.close()\n",
        "print(len(good_features), \" features\")\n",
        "\n",
        "print(\"Performing hyperparameter selection...\")\n",
        "# Hyperparameter selection loop\n",
        "score_hist = []\n",
        "Xt = sparse.hstack([Xts[j] for j in good_features]).tocsr()\n",
        "if learner == 'NB':\n",
        "    Cvals = [0.001, 0.003, 0.006, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.1]\n",
        "else:\n",
        "    Cvals = np.logspace(-4, 4, 15, base=2)  # for logistic\n",
        "for C in Cvals:\n",
        "    if learner == 'NB':\n",
        "        model.alpha = C\n",
        "    else:\n",
        "        model.C = C\n",
        "    score = cv_loop(Xt, y, model, N)\n",
        "    score_hist.append((score,C))\n",
        "    print(\"C: %f Mean AUC: %f\" %(C, score))\n",
        "bestC = sorted(score_hist)[-1][1]\n",
        "print(\"Best C value: %f\" % (bestC))\n",
        "\n",
        "print(\"Performing One Hot Encoding on entire dataset...\")\n",
        "Xt = np.vstack((X_train_all[:,good_features], X_test_all[:,good_features]))\n",
        "Xt, keymap = OneHotEncoder(Xt)\n",
        "X_train = Xt[:num_train]\n",
        "X_test = Xt[num_train:]\n",
        "\n",
        "if learner == 'NB':\n",
        "    model.alpha = bestC\n",
        "else:\n",
        "    model.C = bestC\n",
        "\n",
        "print(\"Training full model...\")\n",
        "print(\"Making prediction    and saving results...\")\n",
        "model.fit(X_train, y)\n",
        "preds = model.predict_proba(X_test)[:,1]\n",
        "create_test_submission(submit, preds)\n",
        "preds = model.predict_proba(X_train)[:,1]\n",
        "create_test_submission('Train'+submit, preds)"
      ]
    }
  ]
}